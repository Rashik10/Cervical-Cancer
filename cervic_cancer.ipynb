{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel files\n",
    "images_df = pd.read_excel('./data/Cases_Images.xlsx')\n",
    "meta_df = pd.read_excel('./data/Cases_Meta_data.xlsx', header=1)\n",
    "\n",
    "images_df = images_df.iloc[:, :2]\n",
    "meta_df = meta_df.iloc[:, :3]\n",
    "\n",
    "meta_df.rename(columns={\"Case Number\": \"Case_Number\", \"HPV\": \"Label\"}, inplace=True)\n",
    "images_df.rename(columns={\"Case Number\": \"Case_Number\"}, inplace=True)\n",
    "\n",
    "# Merge the dataframes on 'Case_Number'\n",
    "# Since images_df has multiple rows for the same Case Number, the merge will duplicate meta_df's data to match.\n",
    "merged_df = pd.merge(images_df, meta_df, on='Case_Number', how='left')\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class labels (assuming the label column is 'Label' and contains 'positive', 'negative', 'unknown')\n",
    "class_labels = merged_df['Label'].unique()\n",
    "\n",
    "# Create directories for train, val, and test\n",
    "base_dir = 'processed_data'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for label in class_labels:\n",
    "        os.makedirs(os.path.join(base_dir, split, label), exist_ok=True)\n",
    "\n",
    "# Split the data into train, val, and test\n",
    "train_df, test_df = train_test_split(merged_df, test_size=0.1, stratify=merged_df['Label'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['Label'], random_state=42)  # 0.9 * 0.2 = 0.18\n",
    "\n",
    "# Function to copy images to the corresponding directory\n",
    "def copy_images(df, split, base_dir):\n",
    "    for index, row in df.iterrows():\n",
    "        case_number = row['Case_Number']  # Assuming 'Case_Number' is the correct column name in the merged_df\n",
    "        label = row['Label']\n",
    "        \n",
    "        # Format the case directory name with leading zeros\n",
    "        case_dir = f'Case {int(case_number):03d}'\n",
    "        \n",
    "        # Construct the path to the case folder\n",
    "        case_path = os.path.join('./data', case_dir)  # Adjust this path as necessary\n",
    "        \n",
    "        # Ensure the case folder exists before proceeding\n",
    "        if os.path.exists(case_path):\n",
    "            for img_name in os.listdir(case_path):\n",
    "                src = os.path.join(case_path, img_name)\n",
    "                dst = os.path.join(base_dir, split, label, img_name)\n",
    "                shutil.copy(src, dst)\n",
    "        else:\n",
    "            print(f\"Warning: Directory {case_path} does not exist. Skipping this case.\")\n",
    "\n",
    "# Copy the images\n",
    "copy_images(train_df, 'train', 'processed_data')\n",
    "copy_images(val_df, 'val', 'processed_data')\n",
    "copy_images(test_df, 'test', 'processed_data')\n",
    "\n",
    "print(\"Data has been processed and split into train, validation, and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "data_dir = 'processed_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final layer to match the number of classes (3 classes: positive, negative, unknown)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=25):\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_acc_history.append(epoch_acc)\n",
    "            else:\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "                # Deep copy the model\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc_history, val_acc_history, train_loss_history, val_loss_history\n",
    "\n",
    "model, train_acc, val_acc, train_loss, val_loss = train_model(model, criterion, optimizer, dataloaders, num_epochs=25)\n",
    "\n",
    "\n",
    "def test_model(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    test_loss = running_loss / dataset_sizes['test']\n",
    "    test_acc = running_corrects.double() / dataset_sizes['test']\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f} Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    return all_labels, all_preds, test_acc, test_loss\n",
    "\n",
    "all_labels, all_preds, test_acc, test_loss = test_model(model, dataloaders['test'], criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyTorch tensors to NumPy arrays for plotting\n",
    "train_acc = [acc.cpu().numpy() for acc in train_acc]\n",
    "val_acc = [acc.cpu().numpy() for acc in val_acc]\n",
    "train_loss = [loss for loss in train_loss]  # Loss is typically already a Python float\n",
    "val_loss = [loss for loss in val_loss]  # Loss is typically already a Python float\n",
    "test_acc = test_acc.cpu().numpy()  # Convert test accuracy to NumPy\n",
    "test_loss = test_loss  # Test loss is typically already a Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss\n",
    "epochs_range = range(len(train_acc))  # Use the actual number of epochs\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_acc, label='Train Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.hlines(test_acc, xmin=0, xmax=len(epochs_range)-1, colors='r', label='Test Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training, Validation, and Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.savefig('accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_loss, label='Train Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.hlines(test_loss, xmin=0, xmax=len(epochs_range)-1, colors='r', label='Test Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training, Validation, and Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.savefig('Loss.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print('Confusion Matrix')\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
